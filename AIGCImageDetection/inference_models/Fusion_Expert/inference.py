# -*- coding: utf-8 -*-
"""
Fusion_Expert_RINE Ascend æ¨ç†è„šæœ¬ï¼ˆé€‚é… utils ç›®å½•ç»“æ„ï¼‰
"""

import sys
import os
import cv2
import os
from pathlib import Path
from collections import Counter


# ç¡®ä¿å½“å‰ç›®å½•åŠ å…¥æ¨¡å—æœç´¢è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
import torchvision.transforms as transforms
from pytorch_wavelets import DWTForward
import acl

# ä» utils ç›®å½•å¯¼å…¥ Ascend ACL å·¥å…·ç±»
import acl
import utils.acllite_utils as acl_utils
from utils.acllite_model import AclLiteModel
from utils.acllite_resource import AclLiteResource
from utils.smart_detection import saliency_based_crop, edge_density_crop, entropy_based_crop

# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
SCRIPT_DIR = Path(__file__).parent.absolute()
PROJECT_ROOT = SCRIPT_DIR # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´å±‚çº§

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•ä¸ºå·¥ä½œç›®å½•
#os.chdir(PROJECT_ROOT)


class Fusion_Expert_Ascend:
    def __init__(self, model_dir= "./om_models"):
        self.model_dir = PROJECT_ROOT / model_dir

        # åˆå§‹åŒ– ACL èµ„æºï¼ˆä½¿ç”¨ utils æä¾›çš„ AclLiteResourceï¼‰
        print("ğŸš€ åˆå§‹åŒ– ACL èµ„æº...")
        self.acl_resource = AclLiteResource()
        self.acl_resource.init()


        # å›¾åƒé¢„å¤„ç†å˜æ¢
        self.common_transform = transforms.Compose([
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean = [0.485, 0.456, 0.406],
               std = [0.229, 0.224, 0.225])  
        ])

        # åŠ è½½ OM æ¨¡å‹
        self._load_models()

    def _load_models(self):
        """åŠ è½½ OM æ¨¡å‹ï¼šFusion_Expert"""
        print("ğŸ”§ åŠ è½½ OM æ¨¡å‹...")
    
        # Fusion_Expert æ¨¡å‹
        model_path = os.path.join(self.model_dir, "fusion_expert.om")
        model_size = os.path.getsize(model_path) / 1024 / 1024
        self.model = AclLiteModel(model_path)
        
        print(f"âœ… å·²åŠ è½½ fusion_expert.om, {model_size:.2f} MB")


    def preprocess_image(self, image_path):
        """
        å›¾åƒé¢„å¤„ç†ï¼šä¸­å¿ƒè£å‰ª224+ImageNetæƒé‡æ ‡å‡†åŒ–
        """
        # ç¡®ä¿å¤„ç†æ‰€æœ‰å¯èƒ½çš„è¾“å…¥ç±»å‹
        if isinstance(image_path, (str, Path)):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²æˆ–Pathå¯¹è±¡ï¼Œæ‰“å¼€å›¾åƒ
            if not os.path.exists(str(image_path)):
                raise FileNotFoundError(f"å›¾åƒä¸å­˜åœ¨: {image_path}")
            image = Image.open(str(image_path)).convert('RGB')
        elif isinstance(image_path, Image.Image):
            # å¦‚æœå·²ç»æ˜¯PIL Imageï¼Œç›´æ¥ä½¿ç”¨
            image = image_path
            if image.mode != 'RGB':
                image = image.convert('RGB')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒè¾“å…¥ç±»å‹: {type(image_path)}")
        
        crop_mode = "saliency"
        print(f"crop mode:{crop_mode}")
        
        if crop_mode == "random":
            width, height = image.size
            i = torch.randint(0, height - 256 + 1, size=(1,)).item() if height > 256 else 0
            j = torch.randint(0, width - 256 + 1, size=(1,)).item() if width > 256 else 0
            
            cropped_image = image.crop((j, i, j + 256, i + 256))
            
        
        elif crop_mode == "saliency":
            cropped_image = saliency_based_crop(image, 224)
            
        elif crop_mode == "edge_density":
            cropped_image = edge_density_crop(image)
            
        elif crop_mode == "entropy":
            cropped_image = entropy_based_crop(image)
            
        elif crop_mode == None:    
            image_tensor = self.common_transform(image).unsqueeze(0)  # [1, 3, 256, 256]
        #cropped_image.save("region.png")
        #print(f"âœ… è£å‰ªåŒºåŸŸå·²ä¿å­˜: region.png")    

        # åŸºç¡€é¢„å¤„ç†ï¼šä½¿ç”¨è£å‰ªåçš„å›¾åƒç»§ç»­å¤„ç†
        if crop_mode:
           image_tensor = transforms.ToTensor()(cropped_image) # [1, 3, 256, 256]
           image_tensor =  transforms.Normalize(mean = [0.485, 0.456, 0.406],
               std = [0.229, 0.224, 0.225]) (image_tensor) .unsqueeze(0)


        #print(f"ğŸ–¼ï¸  åŸå§‹å›¾åƒé¢„å¤„ç†å: {image_tensor.shape}")
        # å…ˆæ‰§è¡Œéšæœºè£å‰ªå¹¶ä¿å­˜
        model_input = image_tensor.squeeze(0).numpy().astype(np.float32)  # [3, 256, 256]


        return model_input

    def body_inference(self, model_input):
 
        # æ¨ç†åˆ†ç±»å™¨
        model_input_batch = np.expand_dims(model_input, axis=0)  # [1, 3, 256, 256]
        logits = self.model.execute([model_input_batch])
        #print(f"logitsï¼š{logits}")
        return logits[0].flatten()  # è¿”å› 2ç»´ logits

    def predict(self, image_path):
        """
        å¯¹å•å¼ å›¾åƒè¿›è¡Œé¢„æµ‹
        """
        try:
            print(f"\nğŸ–¼ï¸  å¤„ç†å›¾åƒ: {image_path}")

            # 1. é¢„å¤„ç†
            model_input = self.preprocess_image(image_path)
            
            # 2. åˆ†ç±»æ¨ç†
            #print("ğŸ” åˆ†ç±»æ¨ç†...")
            logits = self.body_inference(model_input)
            
            print(f"âœ… åˆ†ç±» logits: {logits}")

            # 3. åå¤„ç†ï¼šSoftmax å¾—åˆ°æ¦‚ç‡
            logits_tensor = torch.tensor(logits).unsqueeze(0)  # [1, 2]
            probs = torch.softmax(logits_tensor, dim=1)[0]
            fake_prob = probs[1].item()
            true_prob = probs[0].item()
            #pred_class = 1 if fake_prob > real_prob else 0
           
            
            adaptive_thres = False
            # 5.è®¡ç®—ç½®ä¿¡åº¦
            if adaptive_thres:
                thres = 0.07
                pred_class = 1 if fake_prob > thres else 0
                if pred_class == 0:
                   confidence = (0.5 / thres) * (true_prob - 1 + thres) + 0.5
                else:
                   confidence = 0.5 / (1-thres) * (fake_prob - thres)  + 0.5  
                #print(f"logits_tensor:{logits_tensor}\nprobs:{probs}")
            else:
                pred_class = 1 if fake_prob > 0.5 else 0
                confidence = fake_prob if pred_class == 1 else true_prob
            result = {
                'prediction': 'Fake' if pred_class == 1 else 'True',
                'confidence': confidence
            }

            #print(f"ğŸ‰ é¢„æµ‹ç»“æœ: {result}")
            return result

        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'prediction': 'error',
                'confidence': 0.0,
                'error': str(e)
            }


def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒæ˜¯å¦å®Œæ•´"""
    print("ğŸ§ª æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")

    # æ£€æŸ¥æ¨¡å‹ç›®å½•
    model_dir = PROJECT_ROOT / "om_models"
    if not os.path.exists(model_dir):
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
        return False

    required_models = [
        "fusion_expert.om"
    ]
    missing = []
    size = 0
    for model in required_models:
        path = os.path.join(model_dir, model)
        if not os.path.exists(path):
            missing.append(model)
 
    if missing:
        print(f"âŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {missing}")
        return False

    return True

def Fusion_Expert_Init():
    detector = Fusion_Expert_Ascend(model_dir = PROJECT_ROOT / "om_models")
    return detector
    
def Fusion_Expert_DeInit(detector = None):
    del detector
    print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")
       
# é¡¹ç›®æ¥å£ä¸€:Fusion_Expertå¯¹è¾“å…¥å›¾åƒè¿›è¡Œæ¨ç†, è¾“å‡ºå›¾åƒçœŸå‡ç±»åˆ«å’Œç½®ä¿¡
def ISID(detector, test_img):
    print("=" * 60)
    print("ğŸš€ Fusion_Expert_Ascend æ¨ç†å¼•æ“å¯åŠ¨")
    print("=" * 60)

    # ç¯å¢ƒæ£€æŸ¥
    if not check_environment():
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶")
        sys.exit(1)

    try:
        print("\nğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
        print(f"ç»å¯¹è·¯å¾„:{os.path.abspath(test_img)}")

        # æµ‹è¯•å›¾åƒè·¯å¾„
        #test_img = "test_data/0_real/0.jpg"
        if os.path.exists(test_img):
            print(f"ğŸ“¸ å¼€å§‹æ¨ç†æµ‹è¯•å›¾åƒ: {test_img}")
            result = detector.predict(test_img)

            print(f"\nğŸ“Š æœ€ç»ˆé¢„æµ‹ç»“æœ:")
            print(f"   ç±»åˆ«: {result['prediction']}")
            print(f"   ç½®ä¿¡åº¦: {result['confidence']:.4f}")
            return result['prediction'], result['confidence']
        else:
            print(f"âš ï¸  æµ‹è¯•å›¾åƒæœªæ‰¾åˆ°: {test_img}")
            print("ğŸ’¡ è¯·å°†æµ‹è¯•å›¾åƒæ”¾å…¥ test_data/ ç›®å½•æˆ–æŒ‡å®šè·¯å¾„")
        

    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸ç»ˆæ­¢: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    #finally:
    #    if detector:
    #        del detector
    #    print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

def extract_frames(video_path, num_frames=8):
    """
    ä»è§†é¢‘ä¸­å‡åŒ€æå– num_frames å¸§
    """
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise IOError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = total_frames / fps

    # å‡åŒ€é‡‡æ ·æ—¶é—´ç‚¹
    if total_frames <= num_frames:
        frame_indices = range(total_frames)
    else:
        frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]

    frames = []
    for i in range(total_frames):
        ret, frame = cap.read()
        if not ret:
            break
        if i in frame_indices:
            # è½¬ä¸º RGB å¹¶è½¬æ¢ä¸º PIL Image æ ¼å¼
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_frame)
            frames.append(pil_image)

    cap.release()
    print(f"âœ… ä»è§†é¢‘ä¸­æå– {len(frames)} å¸§ç”¨äºæ£€æµ‹")
    return frames


def extract_frames(video_path, num_frames=8):
    """
    ä»è§†é¢‘ä¸­å‡åŒ€æå– num_frames å¸§
    """
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise IOError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = total_frames / fps

    # å‡åŒ€é‡‡æ ·æ—¶é—´ç‚¹
    if total_frames <= num_frames:
        frame_indices = range(total_frames)
    else:
        frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]

    frames = []
    for i in range(total_frames):
        ret, frame = cap.read()
        if not ret:
            break
        if i in frame_indices:
            # è½¬ä¸º RGB å¹¶è½¬æ¢ä¸º PIL Image æ ¼å¼
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_frame)
            frames.append(pil_image)

    cap.release()
    print(f"âœ… ä»è§†é¢‘ä¸­æå– {len(frames)} å¸§ç”¨äºæ£€æµ‹")
    return frames

# é¡¹ç›®æ¥å£äºŒ:Fusion_Expertå¯¹è¾“å…¥è§†é¢‘æŠ½å–è‹¥å¹²å¸§è¿›è¡Œæ¨ç†, ç»¼åˆå‡ å¸§æ¨ç†ç»“æœï¼Œè¿”å›è§†é¢‘æ£€æµ‹çœŸå‡ç»“æœå’Œå¹³å‡ç½®ä¿¡åº¦
def VSID(detector, video_path, num_frames=100, threshold=0.5):
    """
    ä¼ªé€ è§†é¢‘æ£€æµ‹ä¸»æ¥å£
    Args:
        detector: å·²åˆå§‹åŒ–çš„æ£€æµ‹å™¨å¯¹è±¡
        video_path (str): è§†é¢‘æ–‡ä»¶è·¯å¾„
        num_frames (int): é‡‡æ ·å¸§æ•°ï¼Œé»˜è®¤ 50
        threshold (float): åˆ¤å®šä¸º fake çš„æ¦‚ç‡é˜ˆå€¼ï¼Œé»˜è®¤ 0.5

    Returns:
        dict: {'prediction': 'fake/real', 'confidence': float, 'fake_prob_avg': float}
    """
    print("=" * 60)
    print("ğŸ¥ Fusion_Expert è§†é¢‘ä¼ªé€ æ£€æµ‹å¯åŠ¨")
    print("=" * 60)

    try:
        # 1. æå–å¸§
        frames = extract_frames(video_path, num_frames=num_frames)
        if len(frames) == 0:
            print("âŒ æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆå¸§")
            return {'prediction': 'error', 'confidence': 0.0, 'error': 'no frames extracted'}

        fake_probs = []  # å­˜å‚¨æ¯ä¸€å¸§è¢«é¢„æµ‹ä¸ºå‡çš„æ¦‚ç‡
        predictions = []  # å­˜å‚¨æ¯ä¸€å¸§çš„é¢„æµ‹ç»“æœï¼ˆåŸºäºé˜ˆå€¼ï¼‰
        confidences = []  # å­˜å‚¨æ¯ä¸€å¸§çš„ç½®ä¿¡åº¦

        # 2. éå†æ¯ä¸€å¸§è¿›è¡Œæ£€æµ‹
        for idx, frame in enumerate(frames):
            print(f"\nğŸ–¼ï¸  å¤„ç†ç¬¬ {idx+1}/{len(frames)} å¸§...")
            result = detector.predict(frame)  # æ³¨æ„ï¼šè¿™é‡Œä¼ å…¥çš„æ˜¯ PIL.Image

            # è·å–è¯¥å¸§è¢«é¢„æµ‹ä¸ºå‡çš„æ¦‚ç‡
            # åœ¨å•å¸§é¢„æµ‹ä¸­ï¼Œfake_prob å¯¹åº” probs[1]
            fake_prob = result['confidence'] if result['prediction'] == 'Fake' else (1 - result['confidence'])
            
            fake_probs.append(fake_prob)
            
            # åŸºäºé˜ˆå€¼åˆ¤æ–­å•å¸§ç»“æœ
            frame_pred = 'Fake' if fake_prob > threshold else 'real'
            predictions.append(frame_pred)
            confidences.append(result['confidence'])

            print(f"   å¸§ {idx+1} é¢„æµ‹: {frame_pred}, å‡æ¦‚ç‡: {fake_prob:.4f}, ç½®ä¿¡åº¦: {result['confidence']:.4f}")

        # 3. è®¡ç®—å¹³å‡å‡æ¦‚ç‡
        avg_fake_prob = np.mean(fake_probs)
        
        # 4. åŸºäºå¹³å‡å‡æ¦‚ç‡è¿›è¡Œæœ€ç»ˆåˆ¤æ–­
        final_pred = 'Fake' if avg_fake_prob > threshold else 'real'
        
        # 5. è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
        if final_pred == 'Fake':
            final_confidence = avg_fake_prob
        else:
            final_confidence = 1 - avg_fake_prob  # çœŸå®è§†é¢‘çš„ç½®ä¿¡åº¦

        final_result = {
            'prediction': final_pred,
            'confidence': float(final_confidence),
            'fake_prob_avg': float(avg_fake_prob),  # å¹³å‡å‡æ¦‚ç‡
            'frame_count': len(fake_probs),
            'fake_frame_count': sum(1 for p in predictions if p == 'Fake'),
            'real_frame_count': sum(1 for p in predictions if p == 'real'),
            'per_frame_fake_probs': [float(p) for p in fake_probs],  # æ¯å¸§çš„å‡æ¦‚ç‡
            'per_frame_predictions': predictions
        }

        print(f"\nğŸ“Š è§†é¢‘æ£€æµ‹ç»Ÿè®¡:")
        print(f"   æ€»å¸§æ•°: {final_result['frame_count']}")
        print(f"   åˆ¤ä¸ºå‡çš„å¸§æ•°: {final_result['fake_frame_count']}")
        print(f"   åˆ¤ä¸ºçœŸçš„å¸§æ•°: {final_result['real_frame_count']}")
        print(f"   å¹³å‡å‡æ¦‚ç‡: {avg_fake_prob:.4f}")
        print(f"ğŸ“Š è§†é¢‘æœ€ç»ˆé¢„æµ‹ç»“æœ:")
        print(f"   ç±»åˆ«: {final_result['prediction']}")
        print(f"   ç½®ä¿¡åº¦: {final_result['confidence']:.4f}")
        print(f"   å†³ç­–é˜ˆå€¼: {threshold}")

        return final_result

    except Exception as e:
        print(f"âŒ è§†é¢‘æ£€æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'prediction': 'error',
            'confidence': 0.0,
            'error': str(e)
        }


def main():

    #ISID example 1
    #test_img = "test_images/1_fake/example_fake.jpeg"
    #detector = Fusion_Expert_Init()
    #label, confidence = ISID(detector, test_img)
    #Fusion_Expert_DeInit(detector)
    
    #VSID example 2
    detector = Fusion_Expert_Init()
    test_video = "test_videos/reai_3.mp4"  
    final_result = VSID(detector, test_video)  
    Fusion_Expert_DeInit(detector)

if __name__ == "__main__":
    main()