# -*- coding: utf-8 -*-
"""
SAFE_RINE Ascend æ¨ç†è„šæœ¬ï¼ˆé€‚é… utils ç›®å½•ç»“æ„ï¼‰
"""

import sys
import os
import cv2
import os
from collections import Counter
from pathlib import Path


# ç¡®ä¿å½“å‰ç›®å½•åŠ å…¥æ¨¡å—æœç´¢è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
import torchvision.transforms as transforms
from pytorch_wavelets import DWTForward
import acl

# ä» utils ç›®å½•å¯¼å…¥ Ascend ACL å·¥å…·ç±»ï¼ˆåŸ aclliteï¼‰
import utils.acllite_utils as acl_utils
from utils.acllite_model import AclLiteModel
from utils.acllite_resource import AclLiteResource
from utils.smart_detection import saliency_based_crop, edge_density_crop, entropy_based_crop

# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
SCRIPT_DIR = Path(__file__).parent.absolute()
PROJECT_ROOT = SCRIPT_DIR # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´å±‚çº§


class SR_Ascend:
    def __init__(self, model_dir="./SR_om_models"):
        self.model_dir = model_dir

        # åˆå§‹åŒ– ACL èµ„æºï¼ˆä½¿ç”¨ utils æä¾›çš„ AclLiteResourceï¼‰
        print("ğŸš€ åˆå§‹åŒ– ACL èµ„æº...")
        self.acl_resource = AclLiteResource()
        self.acl_resource.init()

        # åˆå§‹åŒ– DWT å˜æ¢å™¨ï¼ˆç”¨äº SAFE ç‰¹å¾æå–ï¼‰
        #self.dwt = DWTForward(J=1, mode='symmetric', wave='bior1.3')

        # å›¾åƒé¢„å¤„ç†å˜æ¢
        self.common_transform = transforms.Compose([
            transforms.CenterCrop(256),
            transforms.ToTensor()
        ])

        # CLIP å›¾åƒå˜æ¢ï¼ˆ224x224 + å½’ä¸€åŒ–ï¼‰
        '''
        self.clip_transform = transforms.Compose([
            transforms.CenterCrop(224),
            transforms.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],
                std=[0.26862954, 0.26130258, 0.27577711]
            )
        ])
        '''

        # åŠ è½½ OM æ¨¡å‹
        self._load_models()

    def _load_models(self):
        """åŠ è½½ä¸‰ä¸ª OM æ¨¡å‹ï¼šSAFEã€CLIPã€åˆ†ç±»å™¨"""
        print("ğŸ”§ åŠ è½½ OM æ¨¡å‹...")
    
        # SAFE ç‰¹å¾æå–æ¨¡å‹
        safe_model_path = os.path.join(self.model_dir, "safe_feature.om")
        safe_size = os.path.getsize(safe_model_path) / 1024 / 1024
        self.safe_model = AclLiteModel(safe_model_path)
        #print(f"âœ… å·²åŠ è½½ safe_feature.om, {safe_size:.2f} MB")

        # CLIP ç‰¹å¾æå–æ¨¡å‹
        clip_model_path = os.path.join(self.model_dir, "clip_feature_linux_aarch64.om")
        clip_size = os.path.getsize(clip_model_path) / 1024 / 1024
        self.clip_model = AclLiteModel(clip_model_path)
        #print(f"âœ… å·²åŠ è½½ clip_feature_linux_aarch64.om, {clip_size:.2f} MB")

        # åˆ†ç±»å™¨æ¨¡å‹
        classifier_path = os.path.join(self.model_dir, "classifier.om")
        classifier_size = os.path.getsize(classifier_path) / 1024 / 1024
        self.classifier_model = AclLiteModel(classifier_path)
        #print(f"âœ… å·²åŠ è½½ classifier.om, {classifier_size:.2f} MB")

        total_size = safe_size + clip_size + classifier_size
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¤§å°{total_size:.2f} MB")
    
    def _preprocess_dwt(self, x, mode='symmetric', wave='bior1.3'):
        '''
        pip install pywavelets pytorch_wavelets
        '''
        from pytorch_wavelets import DWTForward, DWTInverse
        DWT_filter = DWTForward(J=1, mode=mode, wave=wave).to(x.device)
        Yl, Yh = DWT_filter(x)
        return transforms.Resize([x.shape[-2], x.shape[-1]])(Yh[0][:, :, 2, :, :])

    def preprocess_image(self, image_path):
        """
        å›¾åƒé¢„å¤„ç†ï¼šç”Ÿæˆ SAFE å’Œ CLIP ä¸¤è·¯è¾“å…¥
        """
        if isinstance(image_path, str):
            if not os.path.exists(image_path):
                raise FileNotFoundError(f"å›¾åƒä¸å­˜åœ¨: {image_path}")
            image = Image.open(image_path).convert('RGB')
        else:
            image = image_path

        # åŸºç¡€é¢„å¤„ç†ï¼šä¸­å¿ƒè£å‰ªåˆ° 256x256
        image_tensor = self.common_transform(image).unsqueeze(0)  # [1, 3, 256, 256]
        #print(f"ğŸ–¼ï¸  åŸå§‹å›¾åƒé¢„å¤„ç†å: {image_tensor.shape}")
        '''
        crop_mode = "random"
        
        if crop_mode == "random":
            width, height = image.size
            i = torch.randint(0, height - 256 + 1, size=(1,)).item() if height > 256 else 0
            j = torch.randint(0, width - 256 + 1, size=(1,)).item() if width > 256 else 0
            
            cropped_image = image.crop((j, i, j + 256, i + 256))
            
        
        elif crop_mode == "saliency":
            cropped_image = saliency_based_crop(image)
            
        elif crop_mode == "edge_density":
            cropped_image = edge_density_crop(image)
            
        elif crop_mode == "entropy":
            cropped_image = entropy_based_crop(image)
            
        '''
        
        #cropped_image.save("region.png")
        
        # åŸºç¡€é¢„å¤„ç†ï¼šä½¿ç”¨è£å‰ªåçš„å›¾åƒç»§ç»­å¤„ç†
        #image_tensor = transforms.ToTensor()(cropped_image).unsqueeze(0)  # [1, 3, 256, 256]
            
        # SAFE åˆ†æ”¯ï¼šDWT æå– HH é¢‘å¸¦
        safe_input_tensor = self._preprocess_dwt(image_tensor)  # [1, 3, 256, 256]
        safe_input = safe_input_tensor.squeeze(0).numpy().astype(np.float32)  # [3, 256, 256]

        # CLIP åˆ†æ”¯ï¼šè°ƒæ•´ä¸º 224x224 å¹¶å½’ä¸€åŒ–
        #clip_input_tensor = self.clip_transform(image_tensor)  # [1, 3, 224, 224]
        
        clip_input = image_tensor.squeeze(0).numpy().astype(np.float32) # [3, 244, 244]

        #print(f"âœ… é¢„å¤„ç†å®Œæˆ - SAFEè¾“å…¥: {safe_input.shape}, CLIPè¾“å…¥: {clip_input.shape}")
        return safe_input, clip_input

    def extract_safe_feature(self, safe_input):
        """æå– SAFE ç‰¹å¾"""
        safe_input_batch = np.expand_dims(safe_input, axis=0)  # [1, 3, 256, 256]
        output = self.safe_model.execute([safe_input_batch])
        return output[0].flatten()  # è¿”å› 512 ç»´ç‰¹å¾

    def extract_clip_feature(self, clip_input):
        """æå– CLIP ç‰¹å¾"""
        clip_input_batch = np.expand_dims(clip_input, axis=0)  # [1, 3, 224, 224]
        output = self.clip_model.execute([clip_input_batch])
        return output[0].flatten()  # è¿”å› 1024 ç»´ç‰¹å¾

    def classify_features(self, safe_feature, clip_feature):
 
        # æ¨ç†åˆ†ç±»å™¨
        logits = self.classifier_model.execute([safe_feature, clip_feature])
        return logits[0].flatten()  # è¿”å› 2ç»´ logits

    def predict(self, image_path):
        """
        å¯¹å•å¼ å›¾åƒè¿›è¡Œé¢„æµ‹
        """
        try:
            print(f"\nğŸ–¼ï¸  å¤„ç†å›¾åƒ: {image_path}")

            # 1. é¢„å¤„ç†
            safe_input, clip_input = self.preprocess_image(image_path)

            # 2. æå– SAFE ç‰¹å¾
            #print("ğŸ” æå– SAFE ç‰¹å¾...")
            safe_feature = self.extract_safe_feature(safe_input)
            #print(f"âœ… SAFE ç‰¹å¾ç»´åº¦: {safe_feature.shape}")

            # 3. æå– CLIP ç‰¹å¾
            #print("ğŸ” æå– CLIP ç‰¹å¾...")
            clip_feature = self.extract_clip_feature(clip_input)
            #print(f"âœ… CLIP ç‰¹å¾ç»´åº¦: {clip_feature.shape}")

            # 4. åˆ†ç±»
            #print("ğŸ” åˆ†ç±»æ¨ç†...")
            logits = self.classify_features(safe_feature, clip_feature)
            print(f"âœ… åˆ†ç±» logits: {logits}")

            # 5. åå¤„ç†ï¼šSoftmax å¾—åˆ°æ¦‚ç‡
            logits_tensor = torch.tensor(logits).unsqueeze(0)  # [1, 2]
            probs = torch.softmax(logits_tensor, dim=1)[0]
            fake_prob = probs[1].item()
            true_prob = probs[0].item()
            #pred_class = 1 if fake_prob > real_prob else 0

            adaptive_thres = True
            # 5.è®¡ç®—ç½®ä¿¡åº¦
            if adaptive_thres:
                thres = 1e-21
                pred_class = 1 if fake_prob > thres else 0
                if pred_class == 0:
                   confidence = (0.5 / thres) * (true_prob - 1 + thres) + 0.5
                else:
                   confidence = 0.5 / (1-thres) * (fake_prob - thres)  + 0.5  
                #print(f"logits_tensor:{logits_tensor}\nprobs:{probs}")
            else:
                pred_class = 1 if fake_prob > 0.5 else 0
                confidence = fake_prob if pred_class == 1 else true_prob
            result = {
                'prediction': 'Fake' if pred_class == 1 else 'True',
                'confidence': confidence
            }

            print(f"ğŸ‰ é¢„æµ‹ç»“æœ: {result}")
            return result

        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'prediction': 'error',
                'confidence': 0.0,
                'error': str(e)
            }


def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒæ˜¯å¦å®Œæ•´"""
    print("ğŸ§ª æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")

    # æ£€æŸ¥æ¨¡å‹ç›®å½•
    model_dir = PROJECT_ROOT / "./SR_om_models"
    if not os.path.exists(model_dir):
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
        return False

    required_models = [
        "safe_feature.om",
        "clip_feature_linux_aarch64.om",
        "classifier.om"
    ]
    missing = []
    size = 0
    for model in required_models:
        path = os.path.join(model_dir, model)
        if not os.path.exists(path):
            missing.append(model)
 
    if missing:
        print(f"âŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {missing}")
        return False

    return True

def SR_Init():
    detector = SR_Ascend(model_dir = PROJECT_ROOT / "SR_om_models")
    return detector
    
def SR_DeInit(detector = None):
    del detector
    print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")
       

# é¡¹ç›®æ¥å£ä¸€:SAFE_RINEå¯¹è¾“å…¥å›¾åƒè¿›è¡Œæ¨ç†, è¾“å‡ºå›¾åƒçœŸå‡ç±»åˆ«å’Œç½®ä¿¡
def ISID(detector, test_img):
    print("=" * 60)
    print("ğŸš€ SAFE_RINE Ascend æ¨ç†å¼•æ“å¯åŠ¨")
    print("=" * 60)

    # ç¯å¢ƒæ£€æŸ¥
    if not check_environment():
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶")
        sys.exit(1)

    try:
        print("\nğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
        print(f"ç»å¯¹è·¯å¾„:{os.path.abspath(test_img)}")

        # æµ‹è¯•å›¾åƒè·¯å¾„
        #test_img = "test_data/0_real/0.jpg"
        if os.path.exists(test_img):
            print(f"ğŸ“¸ å¼€å§‹æ¨ç†æµ‹è¯•å›¾åƒ: {test_img}")
            result = detector.predict(test_img)

            print(f"\nğŸ“Š æœ€ç»ˆé¢„æµ‹ç»“æœ:")
            print(f"   ç±»åˆ«: {result['prediction']}")
            print(f"   ç½®ä¿¡åº¦: {result['confidence']:.4f}")
            return result['prediction'], result['confidence']
        else:
            print(f"âš ï¸  æµ‹è¯•å›¾åƒæœªæ‰¾åˆ°: {test_img}")
            print("ğŸ’¡ è¯·å°†æµ‹è¯•å›¾åƒæ”¾å…¥ test_data/ ç›®å½•æˆ–æŒ‡å®šè·¯å¾„")
        

    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸ç»ˆæ­¢: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    #finally:
    #    if detector:
    #        del detector
    #    print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")


def extract_frames(video_path, num_frames=8):
    """
    ä»è§†é¢‘ä¸­å‡åŒ€æå– num_frames å¸§
    """
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise IOError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = total_frames / fps

    # å‡åŒ€é‡‡æ ·æ—¶é—´ç‚¹
    if total_frames <= num_frames:
        frame_indices = range(total_frames)
    else:
        frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]

    frames = []
    for i in range(total_frames):
        ret, frame = cap.read()
        if not ret:
            break
        if i in frame_indices:
            # è½¬ä¸º RGB å¹¶è½¬æ¢ä¸º PIL Image æ ¼å¼
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_frame)
            frames.append(pil_image)

    cap.release()
    print(f"âœ… ä»è§†é¢‘ä¸­æå– {len(frames)} å¸§ç”¨äºæ£€æµ‹")
    return frames


def extract_frames(video_path, num_frames=8):
    """
    ä»è§†é¢‘ä¸­å‡åŒ€æå– num_frames å¸§
    """
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise IOError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = total_frames / fps

    # å‡åŒ€é‡‡æ ·æ—¶é—´ç‚¹
    if total_frames <= num_frames:
        frame_indices = range(total_frames)
    else:
        frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]

    frames = []
    for i in range(total_frames):
        ret, frame = cap.read()
        if not ret:
            break
        if i in frame_indices:
            # è½¬ä¸º RGB å¹¶è½¬æ¢ä¸º PIL Image æ ¼å¼
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_frame)
            frames.append(pil_image)

    cap.release()
    print(f"âœ… ä»è§†é¢‘ä¸­æå– {len(frames)} å¸§ç”¨äºæ£€æµ‹")
    return frames

# é¡¹ç›®æ¥å£äºŒ:SAFE_RINEå¯¹è¾“å…¥è§†é¢‘æŠ½å–è‹¥å¹²å¸§è¿›è¡Œæ¨ç†, ç»¼åˆå‡ å¸§æ¨ç†ç»“æœï¼Œè¿”å›è§†é¢‘æ£€æµ‹çœŸå‡ç»“æœå’Œå¹³å‡ç½®ä¿¡åº¦
def VSID(video_path, num_frames=8, threshold=0.5):
    """
    ä¼ªé€ è§†é¢‘æ£€æµ‹ä¸»æ¥å£
    Args:
        video_path (str): è§†é¢‘æ–‡ä»¶è·¯å¾„
        num_frames (int): é‡‡æ ·å¸§æ•°ï¼Œé»˜è®¤ 8
        threshold (float): åˆ¤å®šä¸º fake çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆç”¨äºå•å¸§åˆ¤æ–­ï¼Œå¯é€‰ï¼‰

    Returns:
        dict: {'prediction': 'fake/real', 'confidence': float}
    """
    print("=" * 60)
    print("ğŸ¥ SAFE_RINE è§†é¢‘ä¼ªé€ æ£€æµ‹å¯åŠ¨")
    print("=" * 60)

    try:
        # 1. æå–å¸§
        frames = extract_frames(video_path, num_frames=num_frames)
        if len(frames) == 0:
            print("âŒ æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆå¸§")
            return {'prediction': 'error', 'confidence': 0.0, 'error': 'no frames extracted'}

        # 2. åˆå§‹åŒ–æ£€æµ‹å™¨ï¼ˆå¤ç”¨ SID ä¸­çš„æ¨¡å‹ï¼‰
        print("\nğŸ”§ åˆå§‹åŒ– SAFE_RINE æ¨¡å‹...")
        detector = SR_Ascend(model_dir="./SR_om_models")

        predictions = []
        confidences = []

        # 3. éå†æ¯ä¸€å¸§è¿›è¡Œæ£€æµ‹
        for idx, frame in enumerate(frames):
            print(f"\nğŸ–¼ï¸  å¤„ç†ç¬¬ {idx+1}/{len(frames)} å¸§...")
            result = detector.predict(frame)  # æ³¨æ„ï¼šè¿™é‡Œä¼ å…¥çš„æ˜¯ PIL.Image

            pred = result['prediction']
            conf = result['confidence']

            predictions.append(pred)
            confidences.append(conf if pred == 'fake' else -conf)  # fake ç”¨æ­£æ•°ï¼Œreal ç”¨è´Ÿæ•°ä¾¿äºå¹³å‡

            print(f"   å¸§ {idx+1} é¢„æµ‹: {pred}, ç½®ä¿¡åº¦: {result['confidence']:.4f}")

        # 4. ç»¼åˆåˆ¤æ–­
        fake_count = sum(1 for p in predictions if p == 'fake')
        real_count = len(predictions) - fake_count

        # å¤šæ•°æŠ•ç¥¨
        final_pred = 'fake' if fake_count > real_count else 'real'

        # å¹³å‡â€œåŠ æƒç½®ä¿¡åº¦â€ï¼šfake ä¸ºæ­£ï¼Œreal ä¸ºè´Ÿï¼Œå–ç»å¯¹å€¼ååŠ æƒå¹³å‡
        avg_confidence = np.mean([abs(c) for c in confidences])
        
        # æ›´ç²¾ç»†ï¼šæŒ‰æŠ•ç¥¨æ¯”ä¾‹åŠ æƒ
        vote_ratio = fake_count / len(predictions)
        calibrated_conf = avg_confidence * (2 * abs(vote_ratio - 0.5))  # å¼ºåŒ–å¤šæ•°ç¥¨çš„ç½®ä¿¡

        final_result = {
            'prediction': final_pred,
            'confidence': float(calibrated_conf),
            'frame_count': len(predictions),
            'fake_frame_count': fake_count,
            'real_frame_count': real_count,
            'per_frame_predictions': predictions,
            'per_frame_confidences': [float(c) for c in confidences]
        }

        print(f"\nğŸ“Š è§†é¢‘æœ€ç»ˆé¢„æµ‹ç»“æœ:")
        print(f"   ç±»åˆ«: {final_result['prediction']}")
        print(f"   ç½®ä¿¡åº¦: {final_result['confidence']:.4f}")
        print(f"   è¯¦ç»†: {fake_count} å¸§åˆ¤ä¸º fake, {real_count} å¸§åˆ¤ä¸º real")

        return final_result

    except Exception as e:
        print(f"âŒ è§†é¢‘æ£€æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'prediction': 'error',
            'confidence': 0.0,
            'error': str(e)
        }

    finally:
        # æ¸…ç†èµ„æº
        if 'detector' in locals():
            del detector
        print("ğŸ§¹ è§†é¢‘æ£€æµ‹èµ„æºæ¸…ç†å®Œæˆ")

def main():

    #ISID example 1
    test_img = "test_images/1_fake/example_fake.jpeg"
    
    detector = SR_Init()
    label, confidence = ISID(detector, test_img)
    SR_DeInit(detector)
    #VSID example 2
    #test_video = "test_videos/fake_2.mp4"  
    #final_result = VSID(test_video)  

if __name__ == "__main__":
    main()